{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Softmax\n",
    "We underestimated how hard it was going from logistic regression to softmax classification. The main problem is to get the weights structure to interact the right way with the rest of the code as it is now a (785,10) tupple instead of (785,1).\n",
    "We have gone through the code and written pseudocode for what we need to change to get it working, but didn't have time to implement.\n",
    "\n",
    "The one-hot-encoding works as expected.\n",
    "\n",
    "### loss_regularized()\n",
    "The loss function needs to change to equation (8) in the assignment text. The rest can remain unchanged since j(w) = E(w) + lambda*C(w) still.\n",
    "\n",
    "### forward_pass()\n",
    "The forward pass now has a new way of propagating described by the softmax function in equation (6)\n",
    "\n",
    "### the rest\n",
    "The rest of the code should not need changing except doublechecking the new variable-dimensions. The gradient decent expression remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt # Plotting library\n",
    "\n",
    "#mnist.init() #Run only if you do not have the data downloaded\n",
    "X_train, Y_train, X_test, Y_test = mnist.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into smaller development sets\n",
    "X_train_orig = X_train[:20000]\n",
    "Y_train_orig = Y_train[:20000]\n",
    "X_test_orig = X_test[8000:]\n",
    "Y_test_orig = Y_test[8000:]\n",
    "\n",
    "\n",
    "#Bias trick on every picture\n",
    "X_train_orig = np.insert(X_train,784,1, axis=1)\n",
    "X_test_orig = np.insert(X_test,784,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shaping the data doing One-hot-encoding\n",
    "\n",
    "X_train = X_train_orig\n",
    "Y_train = np.zeros((Y_train_orig.size,10))\n",
    "X_test = X_test_orig\n",
    "Y_test = np.zeros((Y_test_orig.size,10))\n",
    "\n",
    "\n",
    "for i in range(Y_train_orig.size):\n",
    "    for j in range(10):\n",
    "        if j == Y_train_orig[i]:\n",
    "            Y_train[i,j] = 1\n",
    "\n",
    "\n",
    "\n",
    "#Splitting train into train and validation\n",
    "end = Y_train.size\n",
    "start = int(Y_train.size * 0.9)\n",
    "\n",
    "X_val = X_train[start:end]\n",
    "Y_val = Y_train[start:end]\n",
    "\n",
    "X_train = X_train[:start]\n",
    "Y_train = Y_train[:start]\n",
    "\n",
    "#print(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 500\n",
    "batch_size = 50\n",
    "learning_rate_0 = 0.00001\n",
    "learning_rate = 0.0000001\n",
    "T = 10\n",
    "regularization_lambda = 0.01\n",
    "classes = 10\n",
    "\n",
    "# Tracking variables\n",
    "TRAIN_LOSS = []\n",
    "VAL_LOSS = []\n",
    "TEST_LOSS = []\n",
    "TRAINING_STEP = []\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "TRAIN_CORRECT_PERCENT = []\n",
    "TEST_CORRECT_PERCENT = []\n",
    "VAL_CORRECT_PERCENT = []\n",
    "\n",
    "\n",
    "num_batches_per_epoch = X_train.shape[0] // batch_size\n",
    "print(num_batches_per_epoch)\n",
    "check_step = num_batches_per_epoch // 10 # How often we should calculate validation / train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cross entropy loss function E(w)\n",
    "def loss_regularized(targets, outputs, weights, param):\n",
    "    assert targets.shape == outputs.shape\n",
    "    #the expression for entropy_loss is the only on that needs changing as J(w)=E(w) + lambda*C(w) still\n",
    "    #for all pixels, for all classes: targets*np.log(outputs)\n",
    "    entropy_loss = weights*targets*np.log(outputs)\n",
    "    mean_entropy_loss = -entropy_loss.mean()\n",
    "    \n",
    "    weights= np.reshape(weights,weights.size)\n",
    "    regularized_loss = mean_entropy_loss +param*np.dot(weights,weights)\n",
    "\n",
    "    return regularized_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, w):\n",
    "    #softmax\n",
    "    #instead of the old logistic regression pass we now use softmax function here\n",
    "    a = np.zeros()\n",
    "    for k in range(classes):\n",
    "        a[k] = w[:,k]*X\n",
    "        #output = np.e**(a[k]) / sum of np.e**(a[k']) for all k'\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent_regularized(X, outputs, targets, weights, learning_rate, param):\n",
    "  N = X.shape[0]\n",
    "  assert outputs.shape == targets.shape\n",
    "\n",
    "  #As long as weights has the right shape nothing should need changing here\n",
    "  dw = X * (targets-outputs) + np.transpose(2*param*weights) \n",
    "  dw = dw.mean(axis = 0).reshape(-1, 1) # Normalize gradient w.r.t number of training samples\n",
    "  assert dw.shape == weights.shape, \"dw: {}, w: {}\".format(dw.shape, weights.shape)\n",
    "  weights = weights + learning_rate * dw\n",
    "\n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop(VAL_LOSS):\n",
    "    if VAL_LOSS[len(VAL_LOSS)-1]>VAL_LOSS[len(VAL_LOSS)-2]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anneal_learning_rate(training_it):\n",
    "    alpha_t = learning_rate_0/(1+(training_it/T))\n",
    "    return alpha_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The weight structure now has a different structure to reflect that we have 10 classes\n",
    "#Getting the w-parameter to work correctly with vectorized logic was to hard to do in little time\n",
    "w = np.zeros((num_features, 10))\n",
    "\n",
    "def train_loop(w):\n",
    "  #print(w.shape)\n",
    "  training_it = 0\n",
    "  stop_condition_counter = 0\n",
    "  for epoch in range(epochs):\n",
    "    #shuffle(X_train, Y_train)\n",
    "    X_train_c, Y_train_c = unison_shuffled_copies(X_train, Y_train)\n",
    "    for i in range(num_batches_per_epoch):\n",
    "      \n",
    "      X_batch = X_train_c[i*batch_size:(i+1)*batch_size]\n",
    "      Y_batch = Y_train_c[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "      out = forward_pass(X_batch, w)\n",
    "      learning_rate = anneal_learning_rate(training_it)\n",
    "      w = gradient_decent(X_batch, out, Y_batch, w, learning_rate)\n",
    "      #print(out)\n",
    "      training_it += 1\n",
    "      if i % check_step == 0:\n",
    "        \n",
    "        # Training set\n",
    "        TRAINING_STEP.append(training_it)\n",
    "        \n",
    "        train_out = forward_pass(X_train, w)\n",
    "        train_loss = loss_regularized(Y_train, train_out)\n",
    "        TRAIN_LOSS.append(train_loss)\n",
    "        \n",
    "        val_out = forward_pass(X_val, w)\n",
    "        val_loss = loss_regularized(Y_val, val_out)\n",
    "        VAL_LOSS.append(val_loss)\n",
    "        \n",
    "        test_out = forward_pass(X_test, w)\n",
    "        test_loss = loss_regularized(Y_test, test_out)\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        \n",
    "        #plotting percentage of correct guesses\n",
    "        train_correct = 0\n",
    "        for i in range(Y_train.size):\n",
    "            if (Y_train[i] == 1 and train_out[i] >= 0.5) or (Y_train[i] == 0 and train_out[i] < 0.5):\n",
    "                train_correct +=1\n",
    "        TRAIN_CORRECT_PERCENT.append((train_correct / Y_train.size)*100)\n",
    "        \n",
    "        test_correct = 0\n",
    "        for i in range(Y_test.size):\n",
    "            if (Y_test[i] == 1 and test_out[i] >= 0.5) or (Y_test[i] == 0 and test_out[i] < 0.5):\n",
    "                test_correct +=1\n",
    "        TEST_CORRECT_PERCENT.append((test_correct / Y_test.size)*100)\n",
    "        \n",
    "        val_correct = 0\n",
    "        for i in range(Y_val.size):\n",
    "            if (Y_val[i] == 1 and val_out[i] >= 0.5) or (Y_val[i] == 0 and val_out[i] < 0.5):\n",
    "                val_correct +=1\n",
    "        VAL_CORRECT_PERCENT.append((val_correct / Y_val.size)*100)\n",
    "        \n",
    "        \n",
    "        if early_stop(VAL_LOSS):\n",
    "            stop_condition_counter += 1\n",
    "            if stop_condition_counter > 3:\n",
    "                print(\"Early stopping Triggered!\")\n",
    "                print(training_it)\n",
    "                return w\n",
    "        else:\n",
    "            stop_condition_counter = 0\n",
    "            \n",
    "\n",
    "  return w\n",
    "\n",
    "#w = train_loop(w)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tdt4265)",
   "language": "python",
   "name": "tdt4265"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
